---
title: "Statistical Modelling notebook"
output:
  html_document:
    df_print: paged
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  comment = "",
  results = "hold",
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 4,
  fig.height = 2.6,
  fig.align = "center"
)
```


```{r,message = FALSE, warning = FALSE}
library("car")
library("tidyverse")
library("magrittr")
library("here")
library("janitor")
library("lubridate")
library("gridExtra")
library("readxl")
library("glmnet")
library("Lahman")
library("viridis")
library("lindia")
library("lme4")
library("caret")
library("pROC")

```

# 1. Datasets
a.Create a new dataset called 'Peopledata' that contains all of the variables in the 'People' dataset by

    i. removing all birth information except birthYear and birthCountry and all death information, along with the variable finalGame;
  
    ii. replacing birthCountry is by bornUSA, a logical variable indicating if the player was born in the USA;
    
```{r}
people_data <- People %>%
  mutate(born_usa = if_else(People$birthCountry =="USA",TRUE,FALSE)) %>% 
  dplyr::select(-birthMonth,-birthCity,-birthDay,-birthDate,-birthState,-nameGiven,-finalGame,-retroID,-bbrefID,-birthCountry) %>%  
  dplyr::select(-contains("death"))
```
Create new datasets called Battingdata and Fieldingdata by 

    i. choosing data from the years 1985 and 2015,
    
    ii. selecting only those variables that for those years have fewer than 25 missing cases, 
    
    iii. removing the variable 'G' from the batting dataset and removing the variables "teamID" and "lgID" from both datasets, 
    
    iv. creating a variable in 'Battingdata' called batav which is equal to the number of hits (H) over the number of at bats (AB) if the number of hits >0, and =0 if H=0.
    
```{r}
batting_1985 <- Batting %>%
  filter(yearID == 1985)
batting_2015 <- Batting %>% 
  filter(yearID == 2015)
batting_data <- rbind(batting_1985,batting_2015)

batting_data <-  batting_data %>%
  dplyr::select(-teamID,-lgID,-G) %>% 
  mutate(batav = 
             if_else(batting_data$H > 0 ,batting_data$H/batting_data$AB,0)) 

fielding_1985 <- Fielding %>% 
  filter(yearID == 1985) 
fielding_2015 <- Fielding %>% 
  filter(yearID == 2015) 
fielding_data <- rbind(fielding_1985,fielding_2015)

fielding_data <- fielding_data %>% 
  dplyr::select(-PB,-WP,-SB,-CS,-ZR) %>% 
  dplyr::select(-teamID,-lgID)

```

Create a dataset 'Playerdata' from the dataset 'Salaries' by 
    
    i. selecting data from the years 1985 and 2015, 
    
    ii. adding all distinct variables from the Fieldingdata, Battingdata and Peopledata datasets,
    
    iii. creating a new variable 'allstar' indicating if the player appears anywhere in the AllstarFull dataset,
    
    iv. creating a new variable 'age' equal to each player's age in the relevant year,
    
    iv. dropping incomplete cases from the dataset,
    
    v. dropping unused levels of any categorical variable.
    
```{r}
salaries_1985 <- Lahman::Salaries %>% 
  filter(yearID == 1985) 
salaries_2015 <-  Lahman::Salaries %>%
  filter(yearID == 2015)
player_data <- rbind(salaries_1985,salaries_2015)
player_data <- left_join(player_data,fielding_data)
player_data <- left_join(player_data,batting_data)
player_data <- left_join(player_data,people_data)
player_data <- player_data %>% 
   mutate(all_star = if_else(player_data$playerID %in% AllstarFull$playerID,TRUE,FALSE)) %>%
   mutate(age = yearID - birthYear)
player_data <- player_data[complete.cases(player_data),] %>% 
  droplevels()
player_data <- as_tibble(player_data)
```


Create a dataset called 'TeamSalaries' in which there is a row for each team and each year and the variables are:
    
    i. 'Rostercost' = the sum of all player salaries for the given team in the given year
    
    ii. 'meansalary' = the mean salary for that team that year
    
    iii. 'rostersize' = the number of players listed that year for that team.
    
```{r}
team_salaries <- Lahman::Salaries %>% 
  group_by(teamID,yearID) %>% 
  summarise(rostercost = sum(salary),meansalary = mean(salary),rostersize = n())
```


Create a dataset 'Teamdata' by taking the data from the Teams dataset for the years 1984 to 2016, inclusive and adding to that data the variables in TeamSalaries. Drop any incomplete cases from the dataset.


```{r}
team_data <- Teams %>% 
  filter(yearID >= 1984 & yearID <= 2016) 
team_data <- left_join(team_data,team_salaries ) %>% 
  drop_na() %>% 
  droplevels()
team_data <- as_tibble(team_data)
```

# 2. Simple Linear Regression

Create one plot of mean team salaries over time from 1984 to 2016, and another of the log base 10 of team mean salaries over time from 1984 to 2016.  Give two reasons why a linear model is more appropriate for log base 10 mean salaries than for raw mean salaries.

```{r}

team_data %>% ggplot(aes(yearID,meansalary))+
      geom_point() +
      geom_smooth(method = "lm", se = FALSE,colour="red") +
      labs(x = "Years", y = "Mean Salary") +
      ggtitle("Mean salaries and Years of Football team") +
      theme_classic()
  
```

```{r}

team_data %>% ggplot(aes(yearID,log10(meansalary)))+
      geom_point() +
      geom_smooth(method = "lm", se = FALSE,colour="red") +
      labs(x = "Years", y = "Mean Salary") +
      ggtitle("Log of Mean salaries and Years of Football team") +
      theme_classic()
```
**Distribution of raw mean salaries could be skewed due to the presence of varying number of extreme data points or observations(outliers), so a log transformation would help control that and increase the linearity with the other variable(years).**

**The log of the mean salary against  years plots presents a stronger positive left right linear relationship with a steeper trend line compared to the plot of mean salary against year, furthermore,the log of the mean salary plot against time presents an even spread of data points from left to right with fewer extreme data points on both sides of the trend line while the mean salary plot presents a diverging and uneven spread of data points from left to right with a number of extreme data point especially towards the right side of the plot above the trend line.**


Fit a model of $log_{10}$(meansalary) as a function of yearID.  Write the form of the model and explain what the Multiple R-Squared tells us.

```{r}
linear_model<-lm(log10(meansalary)~yearID,data=team_data)
linear_model
summary(linear_model)
```

$$
{\rm log_{10}(meansalary)} \sim N(-51.2224 + 0.02871 \times  {\rm yearID}, 0.1858).
$$
#This model has a  mulitple r-squared value of  0.4878 and this means that  the variable (yearID) explains 48.78% of the variability of the response  (log of mean salary)


State and evaluate the four assumptions of linear models for this data.

```{r}
linear_model %>%
  gg_diagnose(max.per.page = 1)
```
**Linearity: This assumes that the response and predictor variables have a linear relationship,the residual versus predictor (yearID) plot show the data point are distributed evenly on either side of the central (straight) trend line , there are  this indicates that the assumption for linearity of this model was met.**

**Homoscedasticity: The plot of residual versus predictor (yearID) plot presents an even spread of data from left to right also there are some extreme data points at the lower end of the line but its seems fine overall and the assumption homoscedasticity was met**

**Normality: The histogram of residuals presents a bell shaped curve which indicates a normal distribution, the QQ plot presents a linear relationship of the data points and this again indicates that the assumption normality for this model was met.** 

**Independence:We don’t have a natural order in this data set, so this can not be investigated.**

Plot confidence and prediction bands for this model.  Colour the points according to who won the World Series each year.  Comment on what you find.

```{r}
confint(linear_model)
```


```{r}
team_data %>%
ggplot(aes(x=yearID,y=log10(meansalary),colour=WSWin))+
         geom_point(size=2)+
         geom_smooth(method=lm, color='#2C3E50')+
         #labs()
         theme_classic()
```
**The confidence band around the confidence line is thin and this indicates that we are pretty confident in the confidence band (confidence interval) around the regression line as the confidence band represents the uncertainty around the mean of log salary at each year,also it is very slightly wider at the end than in the middle because we probably have more information around the middle than around the end**

 
```{r}
pred1<-predict(linear_model,interval="prediction") 
myteams<-cbind(team_data,pred1)           
ggplot(myteams,aes(yearID,log10(meansalary),colour=WSWin))+
         geom_point(size=2)+
         geom_smooth(method=lm, color='#2C3E50') +
         #labs
         geom_line(aes(y=lwr), color=2,lty=2) +
         geom_line(aes(y=upr), color=2,lty=2)
```
**The prediction interval is the interval in which we anticipate that 95% of our data point should occur as the prediction band takes account both of the uncertainty in the estimate of the mean and the variance in the residuals , however a  number of data point do not lie within the prediction band, as some occur above the prediction band and some below the prediction band**

Investigate the points that appear above the top prediction band.  What team or teams do they relate to?
```{r}
teams_above_prediction_band <- myteams %>%
mutate(log_of_mean_salary = log10(meansalary)) %>%
filter(log_of_mean_salary  > upr)  
  
teams_above_prediction_band
```

**All of the data points above the prediction band were data points for the team New York Yankees from the year 2002 to 2010.**

# 3. Multiple regression for Count Data

Create a histogram of the number of runs scored for players in the Playerdata dataset so each bar is a single value (0,1,2 runs, etc).  Next create a histogram of the number of runs for all players who have had a hit. Give a domain-based and a data-based reason why it is more reasonable to create a Poisson data for the second set than the first. 

```{r}

player_data %>%
  ggplot(aes(R))+
  geom_histogram(binwidth = 1)+
  labs(x="Number of Runs",y="Frequency",title="Number of runs by players")

```

```{r}
player_data %>% 
  filter( H > 0 ) %>% 
  ggplot(aes(R))+
  geom_histogram(binwidth = 1)+
  labs(x="Number of Runs",y="Frequency",title="Number of runs by players with a Hit")
```

**Domain-based reason: To score a run in baseball(which is what determines how a baseball game is won), the batter must successfully hit the ball before scoring a run,filtering for players that have made at least one hit takes into account batters who have successfully made a hit and successfully scored a run which seems reasonable.**

**Data-based reason: Including players who have not made a hit and scored a run in our model could possibly lead to underdispersion as the model would be taking into account data point that are extreme(the values will be zero) and not relevant, so building a model  with players who have made a hit and scored a run helps us build a model with significant and relevant data points in this context.**


Create a new dataset, OnBase of all players who have had at least one hit.  Transform yearID to a factor.  Construct a Poisson model, glm1, of the number of runs as a function of the number of hits, the year as a factor, position played and player height and age.

```{r}
OnBase <- player_data %>%
  filter(H > 0) %>% 
  mutate(yearID = as_factor(yearID))

glm1 <- glm(R~H+yearID+POS+height+age,data=OnBase,family="poisson")
summary(glm1)
```
$$
{\rm log(number of runs)} \sim N(2.494 + 0.01285 \times  {\rm H} +0.012231 \times  {\rm yearid2015} + -0.01152 \times  {\rm POS2B} +0.005319 \times  {\rm POS3B} + -0.06297 \times  {\rm posc} + 0.06322 \times  {\rm POSOF} + -1.171 \times  {\rm POSP} + -0.01123 \times  {\rm POSSS} + -0.003584 \times  {\rm height} + 0.0104693 \times  {\rm age} ).
$$

Find the p-value for each of the predictor variables in this model using a Likelihood Ratio Test.  What hypothesis does each p-value test, and what mathematically does a p-value tell you about a variable?  Use this definition to say what is meant by the p-value associated to POS and to the p-value associated to height.


```{r}
Anova(glm(R~H+yearID+POS+height+age,data=OnBase,family="poisson"))
```

**The p-value tests the null hypothesis. A low p-value (< 0.05) indicates that the null hypothesis can be rejected, that is a predictor with a low p-value is likely to be a meaningful addition to the model because changes in the predictor's value are related to changes in the response variable, a predictor with a large (insignificant) p-value(> 0.05) suggests that changes in that predictor are not associated with changes in the response variable.**

**The p-value for POS (0.00000000000000022) is so small  and less than 0.05, hence it is statistically significant and this suggests that this is an important variable to the model.There is a probably a relationship between between POS and number of runs scored.**

**The p-value for height (0.10994) is large and greater than 0.05, hence it is statistically insignificant and suggests that height is not an important predictor to the model.**  

State the assumptions of Poisson models and check these where possible.

**Dispersion assumption**
```{r}
plot(glm1,which=3)
abline(h=0.8,col=3)
```

**If variance of response is equal to mean expected value of response,the model is undispersed.if the variance of response is less than mean expected value of resonse, the model is underdispersed.if the variance of the response is greater than the mean expected values of the response, the  model is overdispersed.**

**For this model, the red line is above the above greenline(which is at 0.8) from left to right, this suggests overdispersion of the model.A quassipoisson model might be more appropriate.** 

```{r}
glm2 <- glm(R~H+yearID+POS+height+age,data=OnBase,family="quasipoisson")
summary(glm2)
```
**A quick quasipoissonn model reveals that indeed the the model is overdispersed with dispersion parameter being 3.666552.** 


**Linearity**
```{r}
plot(glm1,which=1)
```
**We can evaluate the linearity of a poisson model by  looking at the residuals(deviance) vs fitted and checking to see if the plot looks fairly flat.With this model, the red line starts out fairly close to the black dotted line on the left side of the plot before making alternating in and out of the black dotted line and moving downwards towards the right side of the plot. This is not looking too great and the linearity assumption for this model can be questioned and investigated for possible insights**


**Distributiion**
```{r}
plot(glm1,which=2)
```

**To evaluate the normal distribution assumption for this model, we will investigate the qq plot for deviance residuals, for this model this is good**

**Independence: We can investigate (deviance) residuals as a function of order of data points and look for evidence of “snaking”. We don’t have a natural order in this data set, so this can not be investigated.**

Now create a new model that includes teamID as a random effect.  Ensure there are no fit warnings.  What does the result tell us about the importance of team on number of runs that players score?  Is this a relatively large or small effect?  How could we check the statistical significance of this effect in R?

```{r}
mixed_model<-glmer(R~H+yearID+POS+height+age+(1|teamID),data=OnBase,family="poisson")
summary(mixed_model)
```
**Importance of team on numbers of runs a player scores is the exponent of ±2 standard deviation tau(0.0965) which is -1.96*tau to +1.96 * tau, hence it will have the following effect, for a player in an average team there is no impact, for an excellent team player it means the player will score on average 1.20821 times as many runs than from an average team, for a player that plays in a poor quality team, it means the player will score on an average 1.20821 times fewer compared to a player from an average team.Mathematically, this effect is relatively small because if an average team player scores 10 runs, it means an excellent team player will score 12 runs and a poor quality team player will score 8 runs, however in a game of baseball one additional home run can win a game.**

What is the mean number of runs could you expect 30-year old, 72 inch tall outfielders playing for the Baltimore Orioles in 2015 with 20 hits to have scored? 

```{r}
predict(mixed_model,newdata = data.frame (H=20, yearID="2015", POS="OF",height=72,age=30,teamID = "BAL"),type="response")
```



# 4.  Lasso Regression for Logistic Regression

Create a new dataset DivWinners by removing all of the variables that are team or park identifiers in the dataset, as well as 'lgID', 'Rank','franchID','divID', 'WCWin','LgWin', and 'WSwin'.
Split the resulting into a training and a testing set so that the variable 'DivWin' is balanced between the two datasets.  Use the seed 123.

```{r}
div_winners <- team_data %>% 
  dplyr::select(-lgID,-Rank,-franchID,-divID,-WCWin,-LgWin,-WSWin,-name) %>% 
  dplyr::select(-contains("team")) %>% 
  dplyr::select(-contains("park"))

div_winners

set.seed(123)
training.samples <- div_winners$DivWin %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- div_winners[training.samples, ]
test.data <- div_winners[-training.samples, ]

train.data
test.data

```



Use the training data to fit a logistic regression model using the 'glmnet' command.  Plot residual deviance against number of predictors.

```{r}
divwin_vector<-as.vector(train.data$DivWin)
div_predict<-model.matrix(~.-1,train.data[,-c(6)])
divfit<- glmnet(div_predict,divwin_vector,family="binomial")
```


```{r}
plot(divfit,xvar="dev",ylim = c(-1,1))
```

How many nonzero model coefficients are needed to explain 50% of the deviance? 60%?  Which coefficients are these in each case?  

```{r}
divfit
```

```{r}
#50%
divfit_50_pct <- coef(divfit, s=0.038030)
divfit_50_pct@Dimnames[[1]][1+divfit_50_pct@i]
```

**2 nonzero model coefficients("W","L" ) will be needed to explain 50% deviance.**


```{r}
#60%
divfit_60_pct<-coef(divfit, s=0.001937)
divfit_60_pct@Dimnames[[1]][1+divfit_60_pct@i]

```
**27 non-zero coefficients will be needed to explain 60% deviance. This is quite a huge leap from 2 (to explain 50% of the deviance )to 27 (to explain 60% of the variance).The 27 non-zero coefficients are :"yearID","Ghome","W","L","AB","H","X2B","X3B""HR","BB","SO","SB","CS","HBP","SF","RA","CG","SV","HA","HRA","BBA","SOA","DP","FP","attendance","PPF","rostersize")**

Now use cross-validation to choose a moderately conservative model.  State the variables you will include.

```{r}
set.seed(123)
divfit_cv<-cv.glmnet(div_predict,divwin_vector,family="binomial")
plot(divfit_cv)
```

```{r}
div_win_3_variables <-coef(divfit,s=divfit_cv$lambda.1se)
div_win_3_variables@Dimnames[[1]][1+div_win_3_variables@i]
```

**The variables to be included are "W", "L" and "attendance".**

e.  [4 + 2 points] Fit the model on the training data, then predict on the testing data.  Plot comparative ROC curves and summarise your findings.

```{r}
div_win_trainmodel_2<-glm(as.factor(DivWin)~W+L,family="binomial",data=train.data)
div_win_trainmodel_2

div_win_trainmodel<-glm(as.factor(DivWin)~W+L+attendance,family="binomial",data=train.data)
div_win_trainmodel
```
**Out of curiosity, i did fit a model (div_win_trainmodel_2) for "W" and "L" both as variables and another model (div_win_trainmodel) with "W","L" and "attendance" as variables.The model with 3 variables (div_win_train_model) did a tiny bit better (with an AIC value of 239.4) than the model (div_win_trainmodel_2) with just 2 variables (AIC output of 239.9). Moving forward, I chose the model with 3 variables.**

```{r}
div_win_predict_train <- predict(div_win_trainmodel, type="response")
div_win_predict_test <- predict(div_win_trainmodel,newdata = test.data,type= "response")
roc_curve<-roc(response=train.data$DivWin,predictor=div_win_predict_train,plot=TRUE,main="ROC Curve for Division Winners",auc=TRUE)
roc_curve_test <- roc(response=test.data$DivWin,predictor= div_win_predict_test,plot=TRUE,auc=TRUE,add=TRUE,col=2)
legend(0,0.4,legend=c("training","testing"),fill=1:2)
```
**These two curves (for train and test data) are close to each other and this is an indication that the model did not overfit the data.**

Find Youden's index for the training data and calculate confusion matrices at this cutoff for both training and testing data.  Comment on the quality of the model for prediction in terms of false negative and false positive rates for the testing data.


```{r}
youdiv_win<-coords(roc_curve,"b",best.method="youden",transpose=TRUE)
youdiv_win
youdiv_win[2]+youdiv_win[3]
```

**The cutoff is 0.1836071**

**Confusion matrix for training data**
```{r}
anewdata=train.data[is.na(train.data$L)==FALSE,]
anewdata$prediv<-ifelse(predict(div_win_trainmodel,newdata=anewdata, type="response")>= 0.1836071,"Y","N")
table(anewdata$prediv,as.factor(anewdata$DivWin))
```

$$
\begin{align}
{\rm sensitivity}(0.1836071) + {\rm specificity}(0.1836071) = 100/106 + 354/418 = 1.790286
\end{align}
$$
**This model aims to identify division winners, on the training data, the model identified 100/106 division winners correctly and misclassified only 6 division winners bring the percentage sensitivity to 94.34%, the model also identified teams 354/418 teams that weren't division winners correctly and misclassified 64 teams that weren't division winners as division winners bringing the percentage specificity to 84.68%. Overall the model performance was really good for both sensitivity and specificity on the training data as the sum of both came to 1.79 which is much more than 1**


**Confusion matrix for test data**
```{r}
anewdata_test=test.data[is.na(test.data$L)==FALSE,]
anewdata_test$prediv<-ifelse(predict(div_win_trainmodel,newdata=anewdata_test, type="response")>= 0.1836071,"Y","N")
table(anewdata_test$prediv,as.factor(anewdata_test$DivWin))
```
$$
\begin{align}
{\rm sensitivity}(0.1836071) + {\rm specificity}(0.1836071) = 25/26 + 87/104 = 1.798077
\end{align}
$$
**On the test data,the model identified 25/26 division winners correctly and misclassified only 1 division winner bringing  the percentage sensitivity to 96.15%, the model also identified 87/104 teams that weren't division winners correctly and misclassified 17 teams that weren't division winners as division winners bringing the percentage specificity to 83.65%. Overall the model performance was also really good for the test data  as the percentage sensitivity increased on the test data.The percentage specificity on the test data dropped by a tiny bit, but overall the model performance was also great on the test data.**
**It is also important to note that with this model performance on the test data, this model did not overfit**